{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4b3237e-6f71-45be-b77e-82165390471d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1553db8a4a70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import GRPODataset, generate_dataset, load_dataset, is_topological_ordering, generate_topological_sort\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import ast\n",
    "import einops\n",
    "from typing import Callable\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29853460-124a-4cf2-a40d-5d32f8a24f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_collate_fn(tokenizer: AutoTokenizer, batch_texts: list[str])->None:\n",
    "    enc = tokenizer(\n",
    "        batch_texts,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    labels = enc[\"input_ids\"].clone() # cloning labels because labels gives the next tokens the model needs to predict. they're automatically right shifted by 1. they must be cloned because if we pass them by reference and then do any in-place modificaitons of them they will also change the toknens\n",
    "\n",
    "    if tokenizer.pad_token_id is not None:\n",
    "        labels[labels == tokenizer.pad_token_id] = -100 # -100 is the default ignore index for attention in hugging face. we're saying 'anywhere that is just padding, don't attend to it.'\n",
    "\n",
    "    enc[\"labels\"] = labels\n",
    "    return enc\n",
    "def generate_datasets_main(dataset_params: dict, n: int, k: int)->None:\n",
    "\n",
    "\n",
    "    for ds in dataset_params.keys():\n",
    "        print(f\"generating dataset {ds}\")\n",
    "        _ = generate_dataset(num_graphs = dataset_params[ds]['size'],\n",
    "                             writefile = dataset_params[ds]['writefile'],\n",
    "                             n=n,\n",
    "                             k=k)\n",
    "        print(f\"wrote dataset to {dataset_params[ds]['writefile']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d8638a-cd26-4912-ac41-480d382c46e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n",
      "10000\n",
      "100\n",
      "Examples:\n",
      "Graph: [(1, 9), (4, 9), (8, 2), (3, 0), (7, 6), (1, 3), (2, 9), (7, 0), (8, 6), (3, 6)]\n",
      "Topological Sort: [8, 2, 7, 5, 4, 1, 3, 6, 0, 9]\n",
      "Graph: [(8, 0), (6, 4), (5, 0), (6, 9), (7, 5), (7, 4), (3, 2), (6, 8), (7, 1), (9, 7)]\n",
      "Topological Sort: [6, 8, 9, 7, 1, 4, 5, 0, 3, 2]\n",
      "Graph: [(7, 0), (5, 4), (7, 1), (9, 4), (4, 8), (4, 7), (6, 8), (3, 0), (6, 0), (4, 1)]\n",
      "Topological Sort: [9, 6, 5, 4, 7, 1, 8, 3, 0, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load / create datasets\n",
    "n=10\n",
    "k=10\n",
    "batch_size=32\n",
    "model_path = \"./models/qwen2.5-0.5b-instruct\"  # change if needed\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "dataset_params = {\n",
    "    'train': {\n",
    "        \"size\": 10000,\n",
    "        \"writefile\":\"./data/train.txt\"\n",
    "    },\n",
    "    'test': {\n",
    "        \"size\": 100,\n",
    "        \"writefile\":\"./data/test.txt\"\n",
    "    },\n",
    "    'prompt_examples': {\n",
    "        \"size\": 3,\n",
    "        \"writefile\":\"./data/prompt_samples.txt\"\n",
    "    }\n",
    "}\n",
    "def handle_load(dataset_params_file: str, key: str, )->None:\n",
    "    if not os.path.exists(dataset_params_file[key]['writefile']):\n",
    "        # generate dataset\n",
    "        generate_datasets_main(dataset_params = dataset_params, n=n, k=k)\n",
    "    ds = load_dataset(dataset_params[key]['writefile'])\n",
    "    return ds\n",
    "        \n",
    "# load raw datasets\n",
    "\n",
    "train_ds_raw = handle_load(dataset_params, key=\"train\")\n",
    "test_ds_raw = handle_load(dataset_params, key=\"test\")\n",
    "prompt_examples_raw = handle_load(dataset_params, key=\"prompt_examples\")\n",
    "\n",
    "# load in model and tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        use_fast=True, # use fast tells hugging face to load the rust implemented backend instead of the python one\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"model is {model}\")\n",
    "\n",
    "# make train / test dataset and dataloader objects\n",
    "train_dataset = GRPODataset(load_dataset(dataset_params['train']['writefile']))\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=lambda batch: tokenize_collate_fn(tokenizer,batch)\n",
    ")\n",
    "test_dataset = GRPODataset(load_dataset(dataset_params['test']['writefile']))\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: tokenize_collate_fn(tokenizer,batch)\n",
    ")\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "\n",
    "# form prompt_examples into a dict with key=graph, value=valid_topological_ordering\n",
    "prompt_examples = \"Examples:\\n\"\n",
    "for i in range(len(prompt_examples_raw)):\n",
    "    # prepare to extend prompt\n",
    "    graph_str = prompt_examples_raw[i]\n",
    "    prompt_examples += (f\"Graph: {graph_str}\\n\")\n",
    "    # get liter pythonic object list[tuple] from str representing it\n",
    "    graph = ast.literal_eval(graph_str) # right now this is a str object representing a graph list[tuple]. this inflates it back into a pythonic object\n",
    "    ordering = generate_topological_sort(graph, n=n)\n",
    "    \n",
    "    # now add topological sort to example\n",
    "    prompt_examples += (f\"Topological Sort: {str(ordering)}\\n\")\n",
    "print(prompt_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00e708a-6f62-4c7b-9cd1-b0b2c7fa74d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[(1, 9), (4, 9), (8, 2), (3, 0), (7, 6), (1, 3), (2, 9), (7, 0), (8, 6), (3, 6)]', '[(8, 0), (6, 4), (5, 0), (6, 9), (7, 5), (7, 4), (3, 2), (6, 8), (7, 1), (9, 7)]', '[(7, 0), (5, 4), (7, 1), (9, 4), (4, 8), (4, 7), (6, 8), (3, 0), (6, 0), (4, 1)]']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(prompt_examples_raw)\n",
    "print(len(prompt_examples_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f675e309-aa1c-440b-9bb6-c252d1d5e93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 375])\n",
      "torch.Size([1, 6])\n",
      "graph strs: ['[(1, 9), (4, 9), (8, 2), (3, 0), (7, 6), (1, 3), (2, 9), (7, 0), (8, 6), (3, 6)]', '[(8, 0), (6, 4), (5, 0), (6, 9), (7, 5), (7, 4), (3, 2), (6, 8), (7, 1), (9, 7)]', '[(7, 0), (5, 4), (7, 1), (9, 4), (4, 8), (4, 7), (6, 8), (3, 0), (6, 0), (4, 1)]', '[(9, 4), (5, 3), (4, 7), (5, 2), (5, 4), (8, 3), (9, 3), (2, 8), (7, 2), (0, 4)]', '[(8, 0), (7, 9), (9, 5), (0, 6), (8, 6), (2, 1), (6, 4), (6, 3), (0, 3), (0, 7)]', '[(6, 9), (2, 8), (3, 0), (2, 4), (1, 9), (8, 7), (3, 4), (0, 8), (8, 9), (2, 5)]', '[(3, 9), (3, 6), (5, 3), (3, 4), (0, 6), (8, 2), (4, 6), (0, 9), (2, 1), (5, 7)]', '[(7, 6), (3, 9), (8, 4), (8, 9), (4, 6), (4, 9), (8, 0), (3, 2), (9, 5), (0, 9)]', '[(0, 6), (2, 1), (4, 7), (1, 3), (8, 9), (4, 3), (0, 1), (1, 6), (4, 0), (7, 3)]', '[(6, 7), (9, 1), (3, 4), (6, 1), (1, 4), (9, 2), (2, 0), (8, 2), (2, 5), (2, 4)]', '[(7, 1), (3, 9), (2, 1), (5, 1), (8, 0), (8, 3), (7, 3), (5, 4), (2, 0), (7, 9)]', '[(3, 8), (3, 9), (6, 5), (2, 9), (1, 8), (0, 4), (6, 0), (2, 3), (5, 7), (2, 1)]', '[(1, 7), (2, 7), (3, 7), (9, 5), (3, 1), (8, 9), (9, 6), (2, 4), (6, 7), (9, 2)]', '[(3, 4), (3, 1), (6, 4), (0, 5), (7, 6), (3, 7), (8, 2), (9, 4), (3, 0), (7, 8)]', '[(3, 8), (4, 8), (7, 5), (4, 1), (4, 5), (2, 8), (9, 8), (0, 1), (0, 3), (7, 2)]', '[(0, 3), (2, 3), (9, 0), (5, 0), (2, 8), (4, 3), (2, 0), (2, 4), (4, 6), (9, 3)]', '[(8, 7), (4, 5), (4, 7), (2, 7), (4, 8), (6, 4), (0, 1), (0, 3), (0, 8), (4, 0)]', '[(3, 6), (5, 1), (5, 2), (3, 8), (1, 4), (0, 2), (5, 9), (7, 8), (9, 2), (7, 1)]', '[(4, 5), (5, 8), (6, 5), (3, 2), (4, 2), (4, 8), (9, 1), (0, 1), (6, 9), (5, 1)]', '[(7, 8), (5, 6), (7, 0), (5, 0), (9, 1), (4, 0), (2, 4), (6, 4), (6, 2), (4, 8)]', '[(7, 6), (5, 9), (5, 1), (5, 2), (0, 6), (8, 9), (9, 4), (7, 4), (4, 2), (3, 6)]', '[(3, 8), (3, 6), (0, 6), (7, 0), (5, 2), (4, 2), (9, 6), (1, 8), (7, 9), (0, 4)]', '[(5, 4), (6, 0), (9, 7), (5, 1), (9, 4), (8, 2), (6, 1), (8, 4), (7, 0), (4, 2)]', '[(7, 4), (8, 2), (5, 6), (6, 1), (7, 8), (8, 0), (6, 4), (7, 5), (7, 0), (7, 9)]', '[(8, 5), (4, 7), (0, 2), (0, 6), (1, 2), (4, 2), (4, 8), (5, 2), (7, 0), (5, 1)]', '[(1, 2), (2, 3), (8, 3), (5, 3), (2, 0), (5, 0), (3, 6), (4, 2), (7, 3), (1, 9)]', '[(6, 3), (4, 6), (0, 6), (5, 1), (4, 9), (1, 6), (5, 9), (1, 4), (5, 7), (2, 8)]', '[(6, 7), (3, 7), (2, 0), (6, 5), (6, 4), (1, 4), (5, 1), (9, 0), (0, 8), (9, 1)]', '[(7, 0), (4, 6), (7, 4), (5, 1), (6, 1), (7, 6), (9, 8), (5, 0), (4, 9), (7, 2)]', '[(0, 6), (0, 4), (3, 7), (3, 0), (3, 5), (0, 1), (9, 5), (7, 1), (3, 4), (2, 4)]', '[(2, 1), (9, 8), (3, 8), (3, 9), (2, 8), (3, 6), (1, 4), (6, 4), (3, 7), (3, 5)]', '[(8, 0), (2, 1), (4, 0), (2, 5), (4, 9), (4, 8), (1, 7), (1, 0), (7, 0), (9, 7)]']\n",
      "graph strs: ['[(4, 2), (0, 7), (9, 2), (0, 2), (4, 9), (9, 3), (9, 5), (0, 1), (4, 6), (9, 1)]', '[(3, 9), (7, 2), (7, 8), (1, 2), (5, 7), (0, 8), (5, 8), (1, 6), (8, 9), (2, 9)]', '[(3, 0), (9, 1), (9, 0), (2, 0), (1, 5), (7, 0), (4, 9), (6, 8), (9, 3), (9, 5)]', '[(7, 2), (0, 1), (1, 5), (4, 6), (4, 8), (7, 5), (0, 2), (9, 5), (0, 3), (1, 2)]', '[(3, 0), (9, 4), (6, 5), (4, 8), (9, 5), (2, 6), (8, 5), (7, 5), (1, 2), (4, 6)]', '[(2, 9), (6, 3), (7, 3), (7, 0), (7, 1), (8, 6), (4, 3), (4, 5), (7, 5), (4, 2)]', '[(2, 9), (8, 9), (8, 0), (8, 5), (8, 1), (0, 1), (8, 2), (3, 2), (5, 2), (4, 1)]', '[(0, 3), (7, 3), (2, 4), (7, 5), (8, 2), (2, 5), (8, 1), (2, 3), (8, 7), (8, 6)]', '[(4, 8), (9, 3), (6, 8), (9, 7), (9, 6), (5, 2), (4, 7), (1, 8), (5, 6), (3, 1)]', '[(2, 4), (7, 3), (0, 9), (9, 5), (0, 8), (2, 5), (9, 8), (7, 6), (8, 1), (9, 2)]', '[(6, 0), (8, 4), (7, 8), (7, 2), (7, 9), (3, 9), (3, 4), (5, 4), (9, 1), (6, 9)]', '[(7, 0), (8, 5), (9, 2), (6, 2), (7, 2), (5, 0), (7, 3), (3, 0), (6, 7), (1, 9)]', '[(4, 8), (3, 1), (4, 5), (4, 1), (5, 2), (9, 1), (0, 3), (2, 9), (2, 3), (0, 7)]', '[(8, 0), (1, 3), (1, 6), (5, 8), (7, 6), (5, 0), (1, 9), (1, 0), (1, 4), (5, 9)]', '[(3, 2), (3, 8), (2, 4), (4, 6), (3, 9), (6, 1), (3, 1), (8, 4), (2, 9), (9, 6)]', '[(2, 3), (4, 6), (0, 2), (2, 4), (0, 8), (0, 6), (0, 9), (9, 5), (7, 3), (7, 5)]', '[(7, 2), (0, 7), (0, 3), (1, 2), (6, 4), (0, 2), (0, 4), (6, 2), (9, 4), (9, 5)]', '[(8, 7), (3, 8), (7, 1), (9, 7), (0, 5), (4, 1), (4, 6), (3, 6), (8, 4), (8, 1)]', '[(6, 5), (5, 7), (1, 9), (8, 9), (6, 0), (0, 4), (8, 5), (9, 5), (0, 9), (4, 7)]', '[(8, 3), (6, 1), (4, 8), (1, 8), (5, 2), (6, 8), (5, 3), (7, 4), (5, 7), (7, 6)]', '[(8, 6), (5, 3), (9, 0), (8, 0), (6, 0), (1, 5), (1, 8), (3, 4), (8, 3), (6, 4)]', '[(0, 6), (0, 2), (1, 2), (6, 3), (0, 9), (1, 9), (0, 8), (7, 3), (5, 9), (6, 9)]', '[(6, 4), (1, 5), (9, 3), (9, 6), (0, 5), (5, 3), (6, 3), (1, 7), (0, 3), (5, 2)]', '[(8, 3), (2, 3), (0, 7), (0, 1), (9, 0), (5, 3), (0, 2), (9, 7), (8, 5), (9, 2)]', '[(1, 7), (6, 8), (5, 7), (1, 4), (6, 7), (9, 8), (4, 7), (9, 5), (1, 6), (4, 0)]', '[(1, 5), (3, 2), (7, 6), (8, 3), (2, 5), (4, 0), (6, 0), (9, 0), (3, 4), (7, 9)]', '[(0, 6), (7, 3), (7, 1), (4, 9), (0, 3), (5, 8), (0, 5), (8, 3), (2, 8), (2, 4)]', '[(2, 0), (6, 2), (1, 3), (2, 3), (4, 8), (9, 6), (8, 1), (4, 7), (9, 2), (2, 1)]', '[(6, 7), (2, 9), (0, 3), (0, 5), (8, 3), (6, 8), (0, 9), (6, 0), (8, 7), (0, 8)]', '[(5, 6), (9, 2), (3, 7), (1, 8), (4, 8), (4, 7), (9, 3), (0, 4), (8, 7), (1, 0)]', '[(5, 8), (3, 1), (5, 3), (2, 9), (0, 5), (5, 9), (6, 9), (0, 2), (0, 4), (3, 7)]', '[(1, 3), (4, 9), (8, 3), (6, 0), (9, 7), (6, 3), (1, 0), (8, 4), (1, 7), (6, 4)]']\n",
      "graph strs: ['[(1, 7), (4, 7), (9, 3), (2, 4), (8, 4), (8, 1), (9, 5), (2, 3), (6, 7), (8, 5)]', '[(1, 7), (2, 8), (7, 4), (9, 8), (5, 8), (0, 1), (4, 6), (9, 6), (5, 6), (5, 1)]', '[(2, 9), (0, 2), (8, 1), (2, 4), (7, 1), (7, 3), (5, 1), (0, 8), (8, 9), (6, 4)]', '[(4, 3), (3, 1), (0, 5), (4, 2), (8, 2), (6, 9), (4, 5), (5, 9), (6, 1), (0, 9)]', '[(1, 7), (5, 1), (3, 8), (6, 4), (5, 7), (2, 3), (4, 7), (0, 8), (0, 3), (9, 7)]', '[(5, 0), (5, 2), (4, 2), (5, 1), (7, 1), (3, 9), (4, 9), (4, 0), (9, 0), (8, 3)]', '[(7, 4), (0, 3), (0, 2), (8, 4), (0, 9), (5, 9), (2, 7), (6, 3), (6, 0), (4, 1)]', '[(9, 0), (4, 9), (1, 5), (3, 1), (8, 5), (4, 7), (9, 8), (7, 5), (6, 5), (4, 5)]', '[(2, 1), (9, 5), (0, 5), (8, 6), (0, 9), (3, 9), (5, 2), (8, 1), (3, 0), (8, 2)]', '[(3, 9), (2, 8), (3, 1), (3, 7), (0, 8), (2, 1), (5, 1), (5, 6), (0, 4), (2, 9)]', '[(4, 8), (7, 8), (7, 9), (6, 8), (5, 7), (1, 9), (3, 7), (5, 2), (2, 8), (6, 1)]', '[(8, 9), (2, 0), (8, 0), (1, 7), (2, 3), (4, 0), (2, 6), (0, 9), (0, 6), (3, 0)]', '[(4, 2), (3, 7), (0, 1), (8, 6), (9, 1), (4, 6), (7, 5), (1, 5), (1, 6), (0, 5)]', '[(9, 8), (6, 1), (1, 5), (0, 3), (7, 9), (8, 2), (7, 8), (0, 1), (9, 0), (9, 6)]', '[(5, 3), (9, 6), (4, 1), (7, 6), (2, 6), (9, 2), (2, 3), (8, 7), (7, 1), (7, 2)]', '[(3, 9), (0, 1), (9, 7), (0, 9), (3, 1), (0, 7), (8, 4), (9, 1), (3, 8), (6, 4)]', '[(7, 2), (3, 9), (0, 8), (2, 8), (5, 9), (8, 4), (0, 6), (0, 3), (9, 6), (5, 0)]', '[(2, 0), (9, 7), (6, 8), (1, 2), (0, 4), (0, 3), (0, 7), (2, 5), (0, 6), (1, 8)]', '[(9, 4), (5, 1), (1, 7), (8, 4), (3, 1), (0, 1), (1, 6), (8, 2), (8, 7), (4, 2)]', '[(5, 3), (5, 0), (2, 3), (6, 8), (6, 0), (4, 8), (5, 2), (2, 9), (1, 9), (3, 8)]', '[(9, 8), (3, 4), (9, 4), (5, 3), (2, 4), (5, 4), (2, 0), (1, 4), (1, 9), (3, 7)]', '[(5, 3), (2, 9), (9, 3), (5, 1), (0, 5), (6, 1), (6, 3), (6, 5), (2, 3), (2, 5)]', '[(4, 5), (6, 5), (8, 0), (2, 7), (2, 0), (4, 1), (9, 8), (1, 7), (9, 0), (3, 0)]', '[(8, 2), (7, 0), (8, 5), (5, 4), (7, 9), (4, 9), (4, 1), (7, 6), (0, 6), (0, 2)]', '[(8, 6), (2, 5), (8, 3), (3, 6), (7, 6), (2, 0), (4, 3), (4, 1), (9, 1), (4, 5)]', '[(8, 0), (2, 1), (8, 2), (6, 0), (0, 2), (5, 3), (3, 1), (6, 5), (5, 2), (5, 8)]', '[(2, 1), (4, 7), (9, 4), (8, 3), (6, 5), (9, 2), (9, 3), (0, 4), (6, 3), (8, 9)]', '[(7, 2), (7, 6), (6, 1), (9, 1), (5, 3), (7, 3), (3, 8), (4, 5), (0, 3), (6, 8)]', '[(5, 4), (4, 8), (9, 8), (7, 4), (1, 0), (1, 3), (1, 7), (2, 3), (5, 8), (8, 6)]', '[(6, 4), (2, 5), (8, 4), (0, 1), (2, 0), (8, 5), (3, 7), (4, 2), (0, 7), (8, 2)]', '[(0, 8), (0, 9), (3, 1), (9, 8), (0, 3), (5, 8), (4, 0), (1, 2), (4, 7), (9, 2)]', '[(3, 9), (7, 5), (6, 0), (4, 5), (0, 2), (1, 9), (0, 8), (2, 9), (4, 2), (7, 8)]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.052083333333333336, 0.4270833333333333)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define GRPO train step funcs\n",
    "def grpo_train_step(model: AutoModelForCausalLM,\n",
    "                    batch: torch.Tensor,\n",
    "                    reward_func: Callable\n",
    "                   )->tuple[torch.Tensor, float]:\n",
    "    # for sample in batch\n",
    "        # form group\n",
    "        # pass group through model\n",
    "        # calculate group score using reward \n",
    "    pass\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_str: str, tokenizer):\n",
    "        self.stop_str = stop_str\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, input_ids: torch.Tensor, scores: torch.Tensor, **kwargs) -> bool:\n",
    "        last_token = self.tokenizer.decode(input_ids[0, -1])\n",
    "        return self.stop_str in last_token\n",
    "\n",
    "def eval_func(model: AutoModelForCausalLM,\n",
    "              loader: DataLoader,\n",
    "              prefix: torch.Tensor,\n",
    "              suffix: torch.Tensor,\n",
    "              eval_metric: Callable,\n",
    "              stopping_criteria: StoppingCriteriaList,\n",
    "              n: int,\n",
    "              tokenizer: AutoTokenizer,\n",
    "              verbose: bool = False) -> tuple[float, float]:\n",
    "\n",
    "    total = 0\n",
    "    correct_metric = 0\n",
    "    correct_format = 0\n",
    "\n",
    "    for idx, batch in enumerate(loader):\n",
    "        batch = batch.to(model.device)\n",
    "        sample = batch['input_ids']\n",
    "        B = sample.shape[0]\n",
    "        total += B\n",
    "\n",
    "        prefix_batch = einops.repeat(prefix, '1 T -> B T', B=B)\n",
    "        suffix_batch = einops.repeat(suffix, '1 T -> B T', B=B)\n",
    "\n",
    "        prompt = torch.cat([prefix_batch, sample, suffix_batch], dim=-1)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                prompt,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=False,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        graph_strs = tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)\n",
    "        print(f\"graph strs: {graph_strs}\")\n",
    "        response_strs = tokenizer.batch_decode(output[:, prompt.shape[-1]-1:], skip_special_tokens=True)\n",
    "\n",
    "        for i in range(B):\n",
    "            graph = ast.literal_eval(graph_strs[i])\n",
    "            try:\n",
    "                top_ord = ast.literal_eval(response_strs[i].strip())\n",
    "                metric = is_topological_ordering(ordering=top_ord, dag=graph, n=n)\n",
    "                formatting_reward = True\n",
    "            except:\n",
    "                if verbose:\n",
    "                    print(f\"qwen outputted illegal response {response_strs[i]}\")\n",
    "                formatting_reward = False\n",
    "                metric = False\n",
    "\n",
    "            if metric:\n",
    "                correct_metric += 1\n",
    "            if formatting_reward:\n",
    "                correct_format += 1\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"[{idx*B+i}] Graph: {graph_strs[i]}\")\n",
    "                print(f\"[{idx*B+i}] Response: {response_strs[i]}\")\n",
    "                print(f\"[{idx*B+i}] Metric: {metric}, formatted correctly? {formatting_reward}\\n\")\n",
    "                print(\"=\" * 80)\n",
    "\n",
    "    pct_metric = correct_metric / total if total > 0 else 0.0\n",
    "    pct_format = correct_format / total if total > 0 else 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n% correct topological ordering: {pct_metric:.2%}\")\n",
    "        print(f\"% correctly formatted: {pct_format:.2%}\")\n",
    "\n",
    "    return pct_metric, pct_format\n",
    "           \n",
    "prefix_text = f\"\"\"\n",
    "    You are doing topological sort of a Directed Acyclic Graph (DAG). You are given an edge list,\n",
    "    a list of tuples where each tuple represents (from_node, to_node). Return a valid topological ordering of the nodes.\n",
    "    \n",
    "    {prompt_examples}\\n\n",
    "    Output ONLY a TOPOLOGICAL SORT in the format of those above, with NO CODE OR EXPLANATIONS. JUST the ordering\n",
    "    Graph:\n",
    "    \"\"\"\n",
    "suffix_text= \"\\nTopological Sort: [\"\n",
    "prefix = tokenizer(prefix_text, return_tensors=\"pt\", add_special_tokens=False)['input_ids']\n",
    "suffix = tokenizer(suffix_text, return_tensors=\"pt\", add_special_tokens=False)['input_ids']\n",
    "print(prefix.shape)\n",
    "print(suffix.shape)\n",
    "\n",
    "model = model.to(device)\n",
    "prefix = prefix.to(device)\n",
    "suffix = suffix.to(device)\n",
    "\n",
    "# ok\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnToken(']', tokenizer)])\n",
    "\n",
    "eval_func(model=model, \n",
    "          loader=test_loader, \n",
    "          prefix=prefix,\n",
    "          suffix=suffix,\n",
    "          tokenizer=tokenizer,\n",
    "          stopping_criteria = stopping_criteria,\n",
    "          eval_metric=is_topological_ordering,\n",
    "          n=n,\n",
    "          verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d1e974a-3b71-4f21-b3ff-cbf27510cca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: [23, 11, 220, 17, 11, 220, 22, 11, 220, 20, 11, 220, 19, 11, 220, 16, 11, 220, 18, 11, 220, 21, 11, 220, 15, 11, 220, 24, 60]\n",
      "decoded individually: ['8', ',', ' ', '2', ',', ' ', '7', ',', ' ', '5', ',', ' ', '4', ',', ' ', '1', ',', ' ', '3', ',', ' ', '6', ',', ' ', '0', ',', ' ', '9', ']']\n",
      "last token id: 60\n",
      "last token decoded: ]\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC - run this alone\n",
    "test_str = \"8, 2, 7, 5, 4, 1, 3, 6, 0, 9]\"\n",
    "tokens = tokenizer.encode(test_str, add_special_tokens=False)\n",
    "print(\"tokens:\", tokens)\n",
    "print(\"decoded individually:\", [tokenizer.decode([t]) for t in tokens])\n",
    "print(\"last token id:\", tokens[-1])\n",
    "print(\"last token decoded:\", tokenizer.decode([tokens[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cf758f4-fc9c-414b-ac69-3a846850a248",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 3.56 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.86 GiB is allocated by PyTorch, and 130.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 126\u001b[0m\n\u001b[1;32m    124\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    125\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 126\u001b[0m \u001b[43mgrpo_train_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# form prompt \u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# project into group \u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# pass group through model (do_sample=True right?) \u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# calculate rewards\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 32\u001b[0m, in \u001b[0;36mgrpo_train_step\u001b[0;34m(model, batch, prefix, stopping_criteria, suffix, tokenizer, group_size, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m group \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrepeat(prompt, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 T -> G T\u001b[39m\u001b[38;5;124m'\u001b[39m, G\u001b[38;5;241m=\u001b[39mgroup_size)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# pass through model\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# decode the otuputs and determine correctness\u001b[39;00m\n\u001b[1;32m     42\u001b[0m graph_str \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(batch[sample_id,:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/torch/utils/_contextlib.py:124\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/transformers/generation/utils.py:2566\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2563\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2565\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2566\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2573\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2576\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2578\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2580\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2581\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/transformers/generation/utils.py:2786\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2783\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 2786\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2787\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2788\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:449\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    431\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/transformers/utils/generic.py:1072\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1072\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:345\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must specify exactly one of input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mand\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m DynamicCache(config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/torch/nn/modules/sparse.py:191\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/postTraining/lib/python3.10/site-packages/torch/nn/functional.py:2567\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2566\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 3.56 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.86 GiB is allocated by PyTorch, and 130.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# okay, your eval harness wors. now you have time to go quickly eat and ten you can do GRPO\n",
    "\n",
    "def teacher_force_logprobs(model, traj, use_grad, prompt_length):\n",
    "    ctx = torch.enable_grad() if use_grad else torch.no_grad()\n",
    "    with ctx:\n",
    "        logits = model(traj)['logits'][:, prompt_length-1:-1, :]\n",
    "        logprobs = F.log_softmax(logits, dim=-1)\n",
    "        logprobs = logprobs.gather(dim=-1, index=traj[:, prompt_length:].unsqueeze(-1))\n",
    "    return logprobs.squeeze(-1)\n",
    "\n",
    "\n",
    "def grpo_train_step(model: AutoModelForCausalLM, \n",
    "                    batch: torch.Tensor, \n",
    "                    prefix: torch.Tensor,\n",
    "                    stopping_criteria: StoppingCriteriaList,\n",
    "                    suffix: torch.Tensor,\n",
    "                    tokenizer: AutoTokenizer,\n",
    "                    group_size: int = 32,\n",
    "                    verbose: bool = False\n",
    "                    ):\n",
    "    \n",
    "    for sample_id in range(batch.shape[0]):\n",
    "\n",
    "        # form prompt\n",
    "        sample = einops.rearrange(batch[sample_id, :], 'T -> 1 T')\n",
    "        prompt = torch.cat([prefix, sample, suffix], dim = -1)\n",
    "\n",
    "        # assemble group from prompt\n",
    "        group = einops.repeat(prompt, '1 T -> G T', G=group_size)\n",
    "\n",
    "        # pass through model\n",
    "        output = model.generate(\n",
    "            group,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # decode the otuputs and determine correctness\n",
    "        graph_str = tokenizer.decode(batch[sample_id,:], skip_special_tokens=True)\n",
    "        graph = ast.literal_eval(graph_str.strip())\n",
    "        response_strs = tokenizer.batch_decode(output[:, prompt.shape[-1]-1:], skip_special_tokens=True)\n",
    "        correct_sorts = 0\n",
    "        formatted_correctly = 0\n",
    "        rewards = torch.zeros(size=(group_size,))\n",
    "\n",
    "        for i in range(group_size):\n",
    "            graph = ast.literal_eval(graph_str.strip())\n",
    "            try:\n",
    "                top_ord = ast.literal_eval(response_strs[i].strip())\n",
    "                metric = is_topological_ordering(ordering=top_ord, dag=graph, n=n)\n",
    "                \n",
    "                formatted_correctly += 1\n",
    "                if metric == True:\n",
    "                    correct_sorts += 1\n",
    "                    rewards[i] = 1.0\n",
    "                else: # in this case the sort was wrong but the response was well-formed. assign a small correctness reward\n",
    "                    rewards[i] = 0.1\n",
    "                if verbose: \n",
    "                    formatting_reward = True\n",
    "                    \n",
    "            except:\n",
    "                if verbose:\n",
    "                    print(f\"qwen outputted illegal response {response_strs[i]}\")\n",
    "                    formatting_reward = False\n",
    "                continue\n",
    "                \n",
    "            if verbose:\n",
    "                print(f\"[group {sample_id}] Graph: {graph_str}\")\n",
    "                print(f\"[group {sample_id}] Response: {response_strs[i]}\")\n",
    "                print(f\"[group {sample_id}] Metric: {metric}, formatted correctly? {formatting_reward}\\n\")\n",
    "                print(\"=\" * 80)\n",
    "                \n",
    "        # calculate group score\n",
    "        print(f\"group {sample_id} correct (%): {100*(correct_sorts / group_size)}, formatting valid (%): {100*(formatted_correctly / group_size)}\")\n",
    "\n",
    "        ##################################\n",
    "        # === Compute advantages ===\n",
    "        A_i = rewards - rewards.mean()  # (G,)\n",
    "        A_i = A_i.to(model.device)\n",
    "\n",
    "        # === Forward pass with gradients to get log-probs ===\n",
    "        prompt_len = prompt.shape[-1]\n",
    "        \n",
    "        # Gradient accumulation to avoid OOM\n",
    "        mini_batch_size = 1\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for start in range(0, group_size, mini_batch_size):\n",
    "            end = min(start + mini_batch_size, group_size)\n",
    "            chunk = output[start:end]\n",
    "            chunk_advantages = A_i[start:end]\n",
    "            \n",
    "            log_probs = teacher_force_logprobs(model=model, traj=chunk, use_grad=True, prompt_length=prompt.shape[-1]-1)\n",
    "            \n",
    "            # Mask out padding\n",
    "            response_tokens = chunk[:, prompt_len:]\n",
    "            pad_mask = (response_tokens != tokenizer.pad_token_id).float()\n",
    "            \n",
    "            # Loss for this chunk\n",
    "            chunk_loss = -(log_probs * chunk_advantages.unsqueeze(-1) * pad_mask).sum()\n",
    "            chunk_tokens = pad_mask.sum().item()\n",
    "            total_tokens += chunk_tokens\n",
    "            \n",
    "            chunk_loss.backward()\n",
    "        \n",
    "        total_loss = total_loss  # for logging only\n",
    "        \n",
    "        print(f\"group {sample_id} | loss computed | advantages: {A_i.tolist()[:5]}...\")\n",
    "\n",
    "\n",
    "    # calculate the loss (this is going to go inside a trainer class)\n",
    "        \n",
    "       \n",
    "    \n",
    "batch = next(iter(train_loader))\n",
    "# batch = batch['input_ids']\n",
    "batch = batch['input_ids'][0,:].unsqueeze(0)\n",
    "batch = batch.to(model.device)\n",
    "grpo_train_step(\n",
    "    model=model,\n",
    "    batch=batch,\n",
    "    prefix=prefix,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    suffix=suffix,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "    \n",
    "\n",
    "    # form prompt \n",
    "    \n",
    "    # project into group \n",
    "    # pass group through model (do_sample=True right?) \n",
    "    # calculate rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb569d-3c51-49e8-a2ae-7fb027b78f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (postTraining)",
   "language": "python",
   "name": "posttraining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
